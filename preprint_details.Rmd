---
title: "R Notebook"
---

# Libraries

```{r}

library(tidyverse)
library(rcrossref)
library(roadoi)
library(rvest)

```

# Retrieve preprint metadata via bioRxiv API

```{r}

# See https://api.biorxiv.org for details
# Note that the API allows querying of both bioRxiv and medRxiv via the 
# 'server' parameter (although this is not documented)

max_results_per_page <- 100
base_url <- "https://api.biorxiv.org/details/"

start <- "2013-11-01"
end <- "2020-04-30"

getPreprintData <- function(server) {
  
  # Make initial request
  url <- paste0(base_url, server, "/", start, "/", end, "/", 0)
  request <- httr::GET(url = url)
  content <- httr::content(request, as = "parsed")
  
  # Determine total number of results and required iterations for paging
  total_results <- content$messages[[1]]$total
  iterations <- ceiling(total_results / max_results_per_page) - 1
  
  data <- content$collection

  for (i in 1:iterations) {
    cursor <- i * max_results_per_page
    url <- paste0(base_url, server, "/", start, "/", end, "/", cursor)
    request <- httr::GET(url)
    content <- httr::content(request, as = "parsed")
    data <- c(data, content$collection)
    Sys.sleep(1) # don't hit the API too hard
  }
  return(data)
}

preprint_data <- purrr::map(c("medrxiv", "biorxiv"), getPreprintData)

parsePreprintData <- function(item) {
  tibble(
    source = item$server,
    doi = item$doi,
    title = item$title,
    authors = item$authors,
    author_corresponding = item$author_corresponding, 
    author_corresponding_institution = item$author_corresponding_institution,
    posted_date = item$date,
    version = item$version,
    license = item$license,
    type = item$type,
    category = item$category,
    abstract = item$abstract,
    is_published = item$published != "NA",
    published_doi = if(item$published == "NA") NA_character_ else item$published
  )
}

# Build a search string containing terms related to COVID-19
search_string <- "coronavirus|covid-19|sars-cov|ncov-2019|2019-ncov|hcov-19|sars-2"

# Parse data to dataframe
preprints_all <- map_dfr(preprint_data, ~ map_df(.x, parsePreprintData)) %>%
  group_by(doi) %>%
  # calculate number of versions of a preprint and number of authors
  mutate(n_versions = n()) %>%
  ungroup() %>%
  # keep the first version record
  filter(version == 1) %>%
  select(-version) %>%
  mutate(
    # clean up DOIs for later matching
    doi = str_trim(str_to_lower(doi)),
    published_doi = str_trim(str_to_lower(published_doi)),
    covid_preprint = case_when(
      str_detect(title, regex(search_string, ignore_case = TRUE)) ~ T, 
      str_detect(abstract, regex(search_string, ignore_case = TRUE)) ~ T,
      T ~ F),
    n_authors = map_int(authors, ~length(strsplit(., split = ";")[[1]]))
    ) %>%
  # some duplicates are included
  distinct() %>%
  # reorder elements
  select(source, doi, posted_date, covid_preprint, title, abstract, n_versions, 
         license, type, category, authors, n_authors, author_corresponding, 
         author_corresponding_institution,  is_published, published_doi)

```

# Save basic metadata of all preprints

```{r}

# Due to file size limits on Github (files must be <100Mb), we split the data 
# into two parts: 2013-11-01 to 2018-12-31, and 2019-01-01 to 2020-04-30
preprints_all %>%
  filter(posted_date <= "2018-12-31") %>%
  write_csv("data/preprints_basic_20131101_20181231.csv")
preprints_all %>%
  filter(posted_date >= "2019-01-01") %>%
  write_csv("data/preprints_basic_20190101_20200430.csv")

```

# Subset preprints for analysis (Sept 2019 - Apr 2020)

```{r}

preprints <- preprints_all %>%
  filter(posted_date >= "2019-09-01",
         posted_date <= "2020-04-30")

```

# Published article metadata via Crossref

```{r}

published_dois <- preprints %>% filter(is_published == T) %>% pull(published_doi)

published_articles_data <- rcrossref::cr_works_(published_dois, parse = T)

parsePublishedArticleData <- function(item) {
  tibble(
    published_doi = item$message$DOI,
    published_title = item$message$title[[1]],
    published_date = lubridate::date(item$message$created$`date-time`),
    published_abstract = if(length(item$message$abstract)) item$message$abstract else NA_character_,
    published_journal = item$message$`container-title`[[1]],
    published_publisher = item$message$publisher
  )
}

published_articles <- map_df(published_articles_data, parsePublishedArticleData) %>%
  mutate(published_doi = str_trim(str_to_lower(published_doi)))

```

# OA information for published articles

```{r}

published_dois <- published_articles %>%
  pull(published_doi)

published_articles_oa <- roadoi::oadoi_fetch(dois = published_dois,
                                             email = "n.fraser@zbw.eu") %>%
  mutate(
    published_doi = doi,
    published_article_is_oa = is_oa,
    published_best_oa_location = map_chr(best_oa_location, ~ if(length(.$host_type)) .$host_type else NA),
    published_journal_is_oa = journal_is_oa,
    published_best_oa_license = map_chr(best_oa_location, ~ if(length(.$license)) .$license else NA)
  ) %>%
  select(published_doi, published_article_is_oa, published_journal_is_oa,
         published_best_oa_location, published_best_oa_license) %>%
  mutate(published_doi = str_trim(str_to_lower(published_doi)))

```

# Disambiguated author affiliations via ROR affiliation matching (preprints only)

```{r}

# some affiliation names (~500) are truncated at 160 characters - in these cases
# we can instead retrieve full affili from the public webpage and match to the 
# corresponding author information from the API results
truncated_institutions <- preprints %>% 
  mutate(inst_name_length = nchar(author_corresponding_institution)) %>% 
  filter(inst_name_length == 160)

# Scrape the bioRxiv and medRxiv websites for affiliation information contained
# in HTML meta tags
getAuthorInstitutions <- function(doi, institution_truncated) {
  
  url <- paste0("https://doi.org/", doi)
  html <- read_html(url)
  
  data <- html %>%
      html_nodes("meta[name='citation_author_institution'][content]") %>%
      html_attr('content')
  
  institutions <- tibble(
    doi = doi,
    institution_truncated = institution_truncated,
    institution = data
  )

  return(institutions)
  
}

a <- map2_dfr(truncated_institutions$doi[1:100],
              truncated_institutions$author_corresponding_institution[1:100],
              getAuthorInstitutions)
```


```{r}

b <- a %>%
  filter(str_trunc(institution, 160, ellipsis = "") == institution_truncated) %>%
  distinct()

a %>%
  filter(!doi %in% b$doi) %>%
  distinct(doi)


```


```{r}
# See https://github.com/ror-community/ror-api
# The full string of the institution name is passed to the ROR institution
# matching API. The API returns a list of possible matches, including a 
# match score (between zero and one, one being a perfect match), and the match
# type (e.g. matching on common terms in institution names, or on....). 
# The first institution returned (which always has the highest score) is retained.

getAuthorAffiliations <- function(doi, institution) {
  
  # update progress bar
  pb$tick()$print()
  
  base_url <- "https://api.ror.org/organizations?affiliation="
  encoded_institution <- URLencode(institution)
  url <- paste0(base_url, encoded_institution)
  request <- httr::GET(url)
  content <- httr::content(request, as = "parsed")
  if(length(content$items)) {
    data <- content$items
    tibble(
      doi = doi,
      institution_match_score = data[[1]]$score,
      institution_match_type = data[[1]]$matching_type,
      institution_match_name = data[[1]]$organization$name,
      institution_match_country_name = data[[1]]$organization$country$country_name,
      institution_match_country_code = data[[1]]$organization$country$country_code
    )
  }
}

# set counter for progress bar
pb <- progress_estimated(length(preprints$doi))

affiliations <- map2_df(preprints$doi,
                        preprints$author_corresponding_institution, 
                        getAuthorAffiliations) %>%
  mutate(doi = str_trim(str_to_lower(doi))) %>%
  distinct()


```

# Merge preprint and published articles dataset

```{r}

preprints_full <- preprints %>%
  #left_join(published_articles, by = "published_doi") %>%
  #left_join(published_articles_oa, by = "published_doi") %>%
  left_join(affiliations, by = "doi") %>%
  mutate(delay_in_days = difftime(published_date,  posted_date, units = c("days"))) %>%
  select(source:author_corresponding_institution,
         institution_match_score:institution_match_country_code,
         is_published, published_doi, published_date, delay_in_days, 
         published_title, published_abstract:published_best_oa_license)

```

# Write final dataset to csv

```{r}

write_csv(preprints_full, "data/preprints_full_20190901_20200430.csv")

```

