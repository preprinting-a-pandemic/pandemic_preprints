---
title: "R Notebook"
---

# Libraries

```{r}

library(tidyverse)
library(rcrossref)
library(roadoi)
library(rvest)
library(fuzzyjoin)
library(rentrez)
library(XML)

```

# Retrieve preprint metadata via bioRxiv API

```{r}

# See https://api.biorxiv.org for details
# Note that the API allows querying of both bioRxiv and medRxiv via the 
# 'server' parameter (although this is not documented)

max_results_per_page <- 100 # max allowable number of results per page
base_url <- "https://api.biorxiv.org/details/"

start <- "2013-11-01"
end <- "2020-04-30"

getPreprintData <- function(server) {
  
  # Make initial request
  url <- paste0(base_url, server, "/", start, "/", end, "/", 0)
  request <- httr::GET(url = url)
  content <- httr::content(request, as = "parsed")
  
  # Determine total number of results and required iterations for paging
  total_results <- content$messages[[1]]$total
  pages <- ceiling(total_results / max_results_per_page) - 1
  
  data <- content$collection
  
  for (i in 1:pages) {
    cursor <- format(i * max_results_per_page, scientific = FALSE) # otherwise page 100000 becomes 1e05, which the api does not recognise
    url <- paste0(base_url, server, "/", start, "/", end, "/", cursor)
    request <- httr::RETRY("GET", url, times = 5, pause_base = 1, pause_cap = 60) # retry if server error
    content <- httr::content(request, as = "parsed")
    if(content$messages[[1]]$status == "ok") {
      data <- c(data, content$collection)
    } else {
      data <- c(data, list("error" = content$messages[[1]]$status))
    }
    Sys.sleep(1) # don't hit the API too hard
  }
  return(data)
}

preprint_data <- purrr::map(c("biorxiv", "medrxiv"), getPreprintData)

```


```{r}

parsePreprintData <- function(item) {
  tibble(
    source = item$server,
    doi = item$doi,
    title = item$title,
    authors = item$authors,
    author_corresponding = item$author_corresponding, 
    author_corresponding_institution = item$author_corresponding_institution,
    posted_date = item$date,
    version = item$version,
    license = item$license,
    type = item$type,
    category = item$category,
    abstract = item$abstract,
    is_published = item$published != "NA",
    published_doi = if(item$published == "NA") NA_character_ else item$published
  )
}

# Build a search string containing terms related to COVID-19
search_string_covid <- "coronavirus|covid-19|sars-cov|ncov-2019|2019-ncov|hcov-19|sars-2"

# Set date of first case of COVID-19
covid_start <- "2019-12-31"

# Parse data to dataframe
preprints_all <- map_dfr(preprint_data, ~ map_df(.x, parsePreprintData)) %>%
  group_by(doi) %>%
  # calculate number of versions of a preprint and number of authors
  mutate(n_versions = n()) %>%
  ungroup() %>%
  # keep the first version record
  filter(version == 1) %>%
  select(-version) %>%
  mutate(
    # clean up DOIs for later matching
    doi = str_trim(str_to_lower(doi)),
    published_doi = str_trim(str_to_lower(published_doi)),
    covid_preprint = case_when(
      str_detect(title, regex(search_string_covid, ignore_case = TRUE)) & posted_date > covid_start ~ T, 
      str_detect(abstract, regex(search_string_covid, ignore_case = TRUE)) & posted_date > covid_start ~ T,
      T ~ F),
    n_authors = map_int(authors, ~length(strsplit(., split = ";")[[1]]))
    ) %>%
  # some duplicates are included
  distinct() %>%
  # reorder elements
  select(source, doi, posted_date, covid_preprint, title, abstract, n_versions, 
         license, type, category, authors, n_authors, author_corresponding, 
         author_corresponding_institution,  is_published, published_doi)

```

# Save basic metadata of all preprints

```{r}

# Due to file size limits on Github (files must be <100Mb), we split the data 
# into two parts: 2013-11-01 to 2018-12-31, and 2019-01-01 to 2020-04-30
preprints_all %>%
  filter(posted_date <= "2018-12-31") %>%
  write_csv("data/preprints_basic_20131101_20181231.csv")
preprints_all %>%
  filter(posted_date >= "2019-01-01") %>%
  write_csv("data/preprints_basic_20190101_20200430.csv")

```

# Subset preprints for analysis (Sept 2019 - Apr 2020)

```{r}

preprints <- preprints_all %>%
  filter(posted_date >= "2019-09-01",
         posted_date <= "2020-04-30")

```

# Additional preprint-publisher pairs

Some preprint-published pairs are given via the bioRxiv API (i.e. referring to the publication notices on the bioRxiv website. We take an additional step to retrieve publication pairs), but these links are incomplete. Here we take another step to increase the number of links by fuzzy matching of preprint titles with titles from our journal articles dataset. We use the R package "fuzzyjoin", using the Levenshtein distance method with a maximum distance of X.

```{r}

journal_articles <- read_csv("data/journal_articles_20200101_20200531.csv") %>%
  select(doi, title)

preprint_article_links <- preprints %>% 
  filter(covid_preprint == T) %>% 
  select(doi, title, published_doi) %>%
  stringdist_inner_join(journal_articles, 
                        by = "title",
                        max_dist = 15,
                        method = "lv",
                        ignore_case = TRUE,
                        distance_col = "match_distance")

# Parameters for match distance and title length are manually tuned by comparing results from fuzzy matching to those derived from bioRxiv publication notices. A match distance of 10 appears good, with a minimum title length of 30 (short titles seem to cause problems)
preprint_article_links <- preprint_article_links %>%
  filter(match_distance <= 10,
         nchar(title.y) >= 30,
         is.na(published_doi)) %>%
  # only keep articles matched to a single distinct published article
  group_by(doi.x) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  filter(n == 1) %>%
  select(doi.x, doi.y) %>%
  rename(doi = doi.x,
         published_doi = doi.y)

```

# Merge fuzzy links to preprint dataset

```{r}

preprints <- preprints %>%  
  left_join(preprint_article_links, by = "doi") %>%
  mutate(
    published_link_type.x = case_when(
      !is.na(published_doi.x) ~ "publication_notice",
      T ~ NA_character_
    ),
    published_link_type.y = case_when(
      !is.na(published_doi.y) ~ "title_match",
      T ~ NA_character_
    )) %>%
  mutate(published_doi = coalesce(published_doi.x, published_doi.y),
         published_link_type = coalesce(published_link_type.x, published_link_type.y)) %>%
  select(-published_doi.x, -published_doi.y, -published_link_type.x, - published_link_type.y) %>%
  mutate(is_published = !is.na(published_doi))

```

# Published article metadata via Crossref

```{r}
 
published_dois <- preprints %>% filter(is_published == T) %>% pull(published_doi)

published_articles_data <- rcrossref::cr_works_(published_dois, parse = T)

parsePublishedArticleData <- function(item) {
  tibble(
    published_doi = item$message$DOI,
    published_title = item$message$title[[1]],
    published_date = lubridate::date(item$message$created$`date-time`),
    published_abstract = if(length(item$message$abstract)) item$message$abstract else NA_character_,
    published_journal = if(length(item$message$`container-title`)) item$message$`container-title`[[1]] else NA_character_,
    published_publisher = item$message$publisher
  )
}

published_articles <- map_df(published_articles_data, parsePublishedArticleData) %>%
  mutate(published_doi = str_trim(str_to_lower(published_doi))) %>%
  filter(published_date <= "2020-05-31") # set maximum publication date of published articles

```

# Merge preprints and published articles data

```{r}

preprints <- preprints %>%
  mutate(
    is_published = case_when(
      !published_doi %in% published_articles$published_doi ~ F,
      T ~ T
    ),
    published_doi = case_when(
      !published_doi %in% published_articles$published_doi ~ NA_character_,
      T ~ published_doi
    ),
    published_link_type = case_when(
      !published_doi %in% published_articles$published_doi ~ NA_character_,
      T ~ published_link_type
    )) %>%
  left_join(published_articles, by = "published_doi") %>%
  mutate(delay_in_days = as.numeric(ymd(published_date) -  ymd(posted_date))) %>%
  distinct()

```

# OA information for published articles

```{r}

published_dois <- preprints %>% filter(is_published) %>% pull(published_doi)

published_articles_oa <- roadoi::oadoi_fetch(dois = published_dois,
                                             email = "n.fraser@zbw.eu") %>%
  mutate(
    published_doi = doi,
    published_article_is_oa = is_oa,
    published_best_oa_location = map_chr(best_oa_location, function(x) if(length(x$host_type)) x$host_type else NA),
    published_journal_is_oa = journal_is_oa,
    published_best_oa_license = map_chr(best_oa_location, function(x) if(length(x$license)) x$license else NA)
  ) %>%
  select(published_doi, published_article_is_oa, published_journal_is_oa,
         published_best_oa_location, published_best_oa_license) %>%
  mutate(published_doi = str_trim(str_to_lower(published_doi)))

```

# Merge preprints and OA data

```{r}

preprints <- preprints %>% 
  left_join(published_articles_oa, by = "published_doi") %>%
  distinct()

```

# PubMed abstracts for published articles

```{r}

published_dois <- preprints %>% filter(is_published) %>% pull(published_doi)

doi_chunks <- split(published_dois, ceiling(seq_along(published_dois)/200))

# id_converter from rcrossref returns a PMID associated with a DOI. It can only take 200 records at a time
published_pmids <- map_dfr(doi_chunks, function(x) id_converter(x)$records) %>%
  filter(!is.na(pmid)) %>%
  select(doi, pmid)

parseAbstract <- function(item) {
  paste0(xpathSApply(item, "//PubmedArticleSet/PubmedArticle/MedlineCitation/Article/Abstract/AbstractText", xmlValue), collapse = ". ")
}

getPubMedData <- function(item) {
  
  # see https://github.com/ropensci/rentrez/issues/100 for getting abstracts from PMIDs
  d <- entrez_fetch("pubmed", id = item$pmid, 
                  rettype= "xml", parsed = T)
  
  return(list(published_doi = item$doi,
              published_abstract_pubmed = parseAbstract(d)))
  
}

getPubMedDataSafely <- safely(getPubMedData)

pubmed_data <- map(published_pmids, getPubMedDataSafely)

parsePubMedData <- function(item) {
  
  if(length(item$result$published_doi)) {
    tibble(
      published_doi = item$result$published_doi,
      published_pubmed_abstract = if(length(item$result$published_abstract_pubmed)) item$result$published_abstract_pubmed else NA_character_)
  } else {
    tibble(
      published_doi = NA_character_,
      published_pubmed_abstract = NA_character_
    )
  }
}

pubmed_abstracts <- map_dfr(pubmed_data, parsePubMedData) %>%
  filter(!is.na(published_pubmed_abstract))

```

# Merge PubMed data with existing data

```{r}

preprints <- preprints %>%
  left_join(pubmed_abstracts, by = "published_doi")

```

# Disambiguated author affiliations via ROR affiliation matching (preprints only)

```{r}

# some affiliation names (~500) are truncated at 160 characters - in these cases
# we can instead retrieve full affili from the public webpage and match to the 
# corresponding author information from the API results
truncated_institutions <- preprints %>% 
  mutate(inst_name_length = nchar(author_corresponding_institution)) %>% 
  filter(inst_name_length == 160)

# Scrape the bioRxiv and medRxiv websites for affiliation information contained
# in HTML meta tags
getAuthorInstitutions <- function(doi, institution_truncated) {
  
  url <- paste0("https://doi.org/", doi)
  html <- read_html(url)
  
  data <- html %>%
      html_nodes("meta[name='citation_author_institution'][content]") %>%
      html_attr('content')
  
  institutions <- tibble(
    doi = doi,
    institution_truncated = institution_truncated,
    institution = data
  )

  return(institutions)
  
}

a <- map2_dfr(truncated_institutions$doi[1:100],
              truncated_institutions$author_corresponding_institution[1:100],
              getAuthorInstitutions)
```


```{r}

b <- a %>%
  filter(str_trunc(institution, 160, ellipsis = "") == institution_truncated) %>%
  distinct()

a %>%
  filter(!doi %in% b$doi) %>%
  distinct(doi)


```


```{r}

# See https://github.com/ror-community/ror-api
# The full string of the institution name is passed to the ROR institution
# matching API. The API returns a list of possible matches, including a 
# match score (between zero and one, one being a perfect match), and the match
# type (e.g. matching on common terms in institution names, or on....). 
# The API-selected best match (field "chosen" = TRUE) is retained if score
# = 1 and match is based on full-phrase matching. Otherwise empty tibble returned.
# These are the most conservative settings customisable.

getAuthorAffiliations <- function(doi, institution) {
  
  # update progress bar
  pb$tick()$print()
  
  base_url <- "https://api.ror.org/organizations?affiliation="
  encoded_institution <- URLencode(institution)
  url <- paste0(base_url, encoded_institution)
  request <- httr::GET(url)
  content <- httr::content(request, as = "parsed")
  if(length(content$items)) {
    data <- content$items
    bind_rows(lapply(data, function(x) unlist(x) %>% t %>% as.data.frame(stringsAsFactors = FALSE))) %>% 
      filter(chosen == "TRUE" & matching_type == "PHRASE" & score == 1) %>%
      slice(1) %>%
      mutate(doi = doi) %>%
      rename(institution_match_score = score,
      institution_match_type = matching_type,
      institution_match_name = organization.name,
      institution_match_country_name = organization.country.country_name,
      institution_match_country_code = organization.country.country_code) %>%
      select(doi, institution_match_score, institution_match_type, institution_match_name, institution_match_country_name, institution_match_country_code) %>%
      as.tibble
  }
}

# set counter for progress bar
pb <- progress_estimated(length(preprints$doi))

affiliations <- map2_df(preprints$doi,
                        preprints$author_corresponding_institution, 
                        getAuthorAffiliations) %>%
  mutate(doi = str_trim(str_to_lower(doi))) %>%
  distinct()


```

# Merge authors data

```{r}

preprints <- preprints %>%
  left_join(affiliations, by = "doi") %>%
  select(source:author_corresponding_institution,
         institution_match_score:institution_match_country_code,
         is_published, published_doi, published_date, delay_in_days, 
         published_title, published_abstract:published_best_oa_license) %>% 
  # Apply manual correction to terms that lead to misdefined first cases, and terms among the top 50 most common terms that are matched incorrectly
  mutate(
    institution_match_country_name = case_when(
      author_corresponding_institution == "Universidad de los Andes" ~ "Colombia",
      author_corresponding_institution == "CAS Key Laboratory of Pathogenic Microbiology and Immunology, Institute of Microbiology, Center for Influenza Research and Early-warning (CASCIRE), Chinese Acad" ~ "China",
      author_corresponding_institution == "Metabiota" ~ "Canada",
      author_corresponding_institution == "Jinan University" ~ "China",
      author_corresponding_institution == "The School of Information Science and Technology, Jinan University" ~ "China",
      author_corresponding_institution == "National Research Council" ~ "Italy",
      author_corresponding_institution == "Northeastern University" ~ "United States",
      author_corresponding_institution == "Georgetown University" ~ "United States",
      author_corresponding_institution == "Institute of Virology, Charite-Universitaetsmedizin Berlin, corporate member of Freie Universitaet Berlin, Humboldt-Universitaet zu Berlin, and Berlin Institute" ~ "Germany",
      author_corresponding_institution == "CNRS" ~ "France",
      author_corresponding_institution == "UCSF" ~ "USA",
      TRUE ~ institution_match_country_name),
    institution_match_country_code = case_when(
      author_corresponding_institution == "Universidad de los Andes" ~ "CO",
      author_corresponding_institution == "CAS Key Laboratory of Pathogenic Microbiology and Immunology, Institute of Microbiology, Center for Influenza Research and Early-warning (CASCIRE), Chinese Acad" ~ "CN",
      author_corresponding_institution == "Metabiota" ~ "CA",
      author_corresponding_institution == "Jinan University" ~ "CN",
      author_corresponding_institution == "The School of Information Science and Technology, Jinan University" ~ "CN",
      author_corresponding_institution == "National Research Council" ~ "IT",
      author_corresponding_institution == "Northeastern University" ~ "US",
      author_corresponding_institution == "Georgetown University" ~ "US",
      author_corresponding_institution == "Institute of Virology, Charite-Universitaetsmedizin Berlin, corporate member of Freie Universitaet Berlin, Humboldt-Universitaet zu Berlin, and Berlin Institute" ~ "DE",
      author_corresponding_institution == "CNRS" ~ "FR",
      author_corresponding_institution == "UCSF" ~ "US",
      TRUE ~ institution_match_country_code)
    )
```

# Preprint Word and Reference Counts

```{r}

word_count <- function(string) {
  sapply(strsplit(string, " "), length)
}

# Scrape the bioRxiv and medRxiv websites for usage stats
getWordCounts <- function(doi) {
  
  base_url = "https://www.biorxiv.org/content/"
  
  url <- paste0(base_url, doi, "v1.full")
  
  html <- read_html(url)
  
  # Get all nodes contained in article full text
  data <- html %>%
    html_nodes(".fulltext-view")
  
  # All paragraphs (anything in "<p>" tags)
  p_text <- data %>%
    html_nodes("p") %>%
    html_text(trim = TRUE) %>%
    str_trim(., side = "both") %>%
    str_remove_all(., "\\[(.*?)\\]") %>%
    str_c(., collapse = " ")
  
  # Abstract
  abstract_text <- data %>%
    html_nodes(xpath="//div[contains(@id, 'abstract-') and @class='abstract']") %>%
    html_text(trim = TRUE) %>%
    str_trim(., side = "both") %>%
    str_remove_all(., "\\[(.*?)\\]") %>%
    str_c(., collapse = " ")
  
  # Figure captions
  fig_text <- data %>% 
    html_nodes(".fig-caption") %>%
    html_text(trim = TRUE) %>%
    str_trim(., side = "both") %>%
    str_remove_all(., "\\[(.*?)\\]") %>%
    str_c(., collapse = " ")
  
  # Table captions
  table_text <- data %>% 
    html_nodes(".table-caption") %>%
    html_text(trim = TRUE) %>%
    str_trim(., side = "both") %>%
    str_remove_all(., "\\[(.*?)\\]") %>%
    str_c(., collapse = " ")
  
  # Acknowledgements
  ack_text <- data %>% 
    html_nodes(xpath="//div[contains(@id, 'ack-') and contains(@class, 'ack')]") %>%
    html_text(trim = TRUE) %>%
    str_trim(., side = "both") %>%
    str_remove_all(., "\\[(.*?)\\]") %>%
    str_c(., collapse = " ")
  
  n_words <- word_count(p_text) - 
              word_count(abstract_text) -
              word_count(fig_text) - 
              word_count(table_text) -
              word_count(ack_text)
    
  n_refs <- html %>%
    html_nodes(xpath="//*[contains(@class, 'ref-cit')]") %>%
    html_text() %>%
    length()

  # update progress bar
  pb$tick()$print()
  
  return(list(
    doi = doi,
    word_count = n_words,
    ref_count = n_refs
  ))
  
}

# Retrieve usage data. Sometimes the bioRxix/medRxiv websites time out and
# return an invalid response. So we conduct the iteration with purrr::safely
# to prevent errors interrupting the process
getWordCountsSafely <- safely(getWordCounts)

biorxiv_preprints <- preprints %>% 
  filter(source == "biorxiv")

# set counter for progress bar
pb <- progress_estimated(length(biorxiv_preprints$doi))

word_counts <- map(biorxiv_preprints$doi, ~ getWordCountsSafely(.))

word_counts_df <- map_dfr(word_counts, ~ tibble(doi = .$result$doi,
                                                n_words = .$result$word_count,
                                                n_refs = .$result$ref_count)) %>%
# remove cases where word and ref counts are zero - these are the result of a
# full text not being available (e.g. if a new version is posted before the initial
# version has had a full text added)
  filter(n_words != 0,
         n_refs != 0)

# Merge with other preprint data
preprints <- preprints %>%
  left_join(word_counts_df, by = "doi") %>%
  select(source:n_versions, n_words, n_refs, license:published_best_oa_license)
  
```

# Write final dataset to csv

```{r}

write_csv(preprints, "data/preprints_full_20190901_20200430.csv")

```