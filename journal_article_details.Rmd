---
title: "R Notebook"
---

```{r}

library(tidyverse)
library(rdimensions) # in development: https://github.com/nicholasmfraser/rdimensions
library(rcrossref)
library(roadoi)

# Retrieve auth token for dimensions
dimensions_login()

```

# Retrieve journal article data from Dimensions

```{r}

# Generate the dimensions query string
dim_q <- '"(\\"coronavirus\\" OR \\"covid-19\\" OR \\"sars-cov\\" OR \\"ncov-2019\\" OR \\"2019-ncov\\" OR \\"hcov-19\\" OR \\"sars-2\\")"'

# Determine how many total results we can expect
search_string <- paste0('search publications in title_abstract_only for ', dim_q, ' where year=2020 return publications')
results <- dimensions_query(search_string)$`_stats`$total_count

# Function for querying dimensions API and returning publication data
getDimensionsData <- function(i) {
  search_string <- paste0('search publications in title_abstract_only for ', dim_q, 
    ' where year=2020 return publications[doi+type+year+title+times_cited+journal+date+publisher] limit 1000 skip ', (i-1)*1000)
  data <- dimensions_query(search_string)$publications
  return(data)
}

# Calculate number of query iterations required (results per page = 1000)
iterations <- ceiling(results/1000)

# Parse Dimensions data to a data frame
parseDimensionsData <- function(item) {
  tibble(
    doi = if(length(item$doi)) item$doi else NA,
    type = item$type,
    year = item$year,
    title = item$title,
    journal = if(length(item$journal$title)) item$journal$title[[1]] else NA,
    times_cited = item$times_cited,
    published_date = item$date,
    publisher = if(length(item$publisher)) item$publisher else NA
  )
}

# Retrieve data
dim_data <- map(1:iterations, getDimensionsData) %>%
  map_dfr(., ~ map_dfr(., parseDimensionsData)) %>%
  # clean doi for matching
  mutate(doi = str_trim(str_to_lower(doi))) %>%
  # We keep only "article" types
  # Note that Dimensions currently mislabels "medRxiv" as "article" types, so
  # these should be removed
  filter(type == "article",
         journal != "medRxiv") %>%
  distinct()

```

# Add publication dates from Crossref

```{r}

# Dimensions provides a field "date" which can be used for publication dates,
# but there appear to be a number of inaccuracies, e.g. >1000 papers with publication
# dates on 2020-01-01. Instead we can use Crossref to return more accurate dates
# using the "created" parameter.

# Caution: A single API call is made for every doi, so for large datasets this
# may take several hours to run

cr_data <- cr_works(dois = dim_data %>% distinct(doi) %>% pull(doi))$data

# Create final dataset
dim_items <- dim_data %>%
  inner_join(cr_data %>% 
               select(doi, created) %>%
               # clean doi for matching
               mutate(doi = str_trim(str_to_lower(doi))),
             by = "doi") %>%
  # select relevant fields
  select(doi, created, title, journal, publisher, times_cited) %>%
  filter(created <= "2020-04-30",
         created >= "2020-01-01") %>%
  mutate(published_date = created) %>%
  distinct()

```

# Detailed OA information from Unpaywall

```{r}

dim_items_oa <- oadoi_fetch(dois = dim_items$doi, email = "n.fraser@zbw.eu") %>%
  mutate(
    doi = str_trim(str_to_lower(doi)),
    article_is_oa = is_oa,
    best_oa_license = map_chr(best_oa_location, ~ if(length(.$license)) .$license else NA),
    best_oa_location = map_chr(best_oa_location, ~ if(length(.$host_type)) .$host_type else NA)
  ) %>%
  select(doi, article_is_oa, journal_is_oa, best_oa_location, best_oa_license)

```

# Create final dataset

```{r}

dim_items %>%
  inner_join(dim_items_oa, by = "doi") %>%
  write_csv("data/journal_articles_20200101_20200430.csv")


```




